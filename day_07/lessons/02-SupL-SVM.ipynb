{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    " \n",
    "A support vector machine is a machine learning model that is able to generalise between two different classes if the set of labelled data is provided in the training set to the algorithm. The main function of the SVM is to check for that hyperplane that is able to distinguish between the two classes.\n",
    "\n",
    " \n",
    "There can be many hyperplanes that can do this task but the objective is to find that hyperplane that has the highest margin that means maximum distances between the two classes, so that in future if a new data point comes that is two be classified then it can be classified easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does SVM Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linearly Separable Data\n",
    "\n",
    "\n",
    "Let us understand the working of SVM by taking an example where we have two classes that are shown is the below image which are a class A: Circle & class B: Triangle. Now, we want to apply the SVM algorithm and find out the best hyperplane that divides the both classes.\n",
    "\n",
    "<img src=\"../data/SVM1.png\" alt=\"SVM1\" style=\"width: 450px;\"/>\n",
    "<img src=\"../data/SVM2.png\" alt=\"SVM2\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "SVM takes all the data points in consideration and gives out a line that is called ‘Hyperplane’ which divides both the classes. This line is termed as ‘Decision boundary’. Anything that falls in circle class will belong to the  class A and vice-versa.\n",
    "\n",
    "<img src=\"../data/SVM3.png\" alt=\"SVM3\" style=\"width: 500px;\"/>\n",
    "\n",
    "There can be many hyperplanes that you can see but the best hyper plane that divides the two classes would be the hyperplane having a large distance from the hyperplane from both the classes. That is the main motive of SVM to find such best hyperplanes.\n",
    "\n",
    "There can be different dimensions which solely depends upon the features we have. It is tough to visualize when the features are more than 3.\n",
    "\n",
    "<img src=\"../data/SVM4.png\" alt=\"SVM4\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Consider we have two classes that are red and yellow class A and B respectively. We need to find the best hyperplane between them that divides the two classes. \n",
    "\n",
    "<img src=\"../data/SVM5.png\" alt=\"SVM5\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Soft margin permits few of the above data points to get misclassified. Also, it tries to make the balance back and forth between finding a hyperplane that attempts to make less misclassifications and maximize the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linearly Non-separable Data\n",
    "\n",
    "<img src=\"../data/SVM6.png\" alt=\"SVM6\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If the data is non linearly separable as shown in the above figure then SVM makes use of kernel tricks to make it linearly separable. The concept of transformation of non-linearly separable data into linearly separable is called Cover’s theorem - “given a set of training data that is not linearly separable, with high probability it can be transformed into a linearly separable training set by projecting it into a higher-dimensional space via some non-linear transformation”. Kernel tricks help in projecting data points to the higher dimensional space by which they became relatively more easily separable in higher-dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Tricks: \n",
    "\n",
    "![SVM7](../data/SVM7.png)\n",
    "\n",
    "\n",
    "Kernel tricks also known as Generalized dot product. Kernel tricks are the way of calculating dot product of two vectors to check how much they make an effect on each other. According to Cover’s theorem the chances of linearly non-separable data sets becoming linearly separable increase in higher dimensions. Kernel functions are used to get the dot products to solve SVM constrained optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Kernel Functions:\n",
    " \n",
    "While using the svm classifier we can take the kernel as ‘linear’ , ’poly’ , ‘rbf’ , ‘sigmoid’. Let us see which are the most used kernels that are polynomial and rbf (Radial Basis Function). You can refer here for documentation that is present on sklearn.\n",
    "\n",
    "\n",
    "- Polynomial Kernel-  The process of generating  new features by using a polynomial combination of all the existing features.\n",
    "\n",
    "\n",
    "- Radial Basis Function(RBF) Kernel-  The process of generating new features calculating the distance between all other dots to a specific dot. One of the rbf kernels that is used widely is the Gaussian Radial Basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree of tolerance in SVM \n",
    " \n",
    "\n",
    "The penalty term that is passed as a hyper parameter in SVM while dealing with both linearly separable and non linear solutions is denoted as ‘C’ that is called as Degree of tolerance. Large value of C results in the more penalty SVM gets when it makes a misclassification. The decision boundary will be dependent on narrow margin and less support vectors.\n",
    "\n",
    " \n",
    "### Pros of SVM\n",
    "\n",
    "-    High stability due to dependency on support vectors and not the data points.\n",
    "\n",
    "-    Does not get influenced by Outliers. \n",
    "\n",
    "-    No assumptions made of the datasets.\n",
    "\n",
    "-    Numeric predictions problem can be dealt with SVM.\n",
    "\n",
    "\n",
    "### Cons of SVM\n",
    "\n",
    " \n",
    "\n",
    "- Blackbox method.\n",
    "\n",
    "- Inclined to overfitting method.\n",
    "\n",
    "- Very rigorous computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "print(\"Labels: \", cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=99)\n",
    "\n",
    "classifier = svm.SVC(kernel='linear')\n",
    " \n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splitted the dataset using train_test-split from sklearn.\n",
    "\n",
    "- For evaluation of the model imported accuracy_score \n",
    "\n",
    "- Initiated object for SVC that is svc_model and fitted the training data to the model. Used ‘linear’ as a kernel.\n",
    "\n",
    "- Made a prediction on y_test and calculated accuracy score on test data  that came out to be 96%.\n",
    "\n",
    " \n",
    "For more advanced examples see [svm-support](../supports/SVM.ipynb)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
